# -*- coding: utf-8 -*-
"""Business_project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k1qcaqSmecREQBIJC6CKnoKcIl_j41qB

# Introduction

Logistics efficiency is vital for business success, but delivery delays remain a major challenge. Using AI and Big Data, companies can predict and reduce these delays. This project uses a real-world logistics dataset from Kaggle to analyze shipment data and build a machine learning model that predicts whether a delivery will be delayed. The aim is to improve operational planning and customer satisfaction through data-driven insights.

# Business Problem

Frequent delivery delays and unpredictable delivery performance increase costs and reduce customer satisfaction in e-commerce/logistics operations.

# Objective

Build a classification model to predict delivery outcome (`label`) for incoming orders so logistics managers can proactively re-route or prioritize shipments or take alternative aproach

**Importing required libraries**
"""
from IPython.display import display
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import pickle
import joblib


df1=pd.read_csv("delay.csv")
variabledescription=pd.read_csv("incom2024_delay_variable_description.csv")


df1=pd.read_csv("delay.csv")
variabledescription=pd.read_csv("incom2024_delay_variable_description.csv")

"""# Data Understanding

This logistics dataset provides a comprehensive look into an organization's sales and order fulfillment process, with the primary objective being to predict delivery outcomes based on various operational and customer-related factors. The data combines customer demographics (e.g., customer_city, customer_segment), detailed order financials (e.g., sales, profit_per_order, order_item_discount_rate), product information (e.g., product_name, category_id), and logistics variables (e.g., shipping_mode, shipping_date). Crucially, the dataset includes the label column, which categorizes delivery outcomes as early arrival (-1), on time (0), or delayed (1). This structure makes the dataset ideal for building a classification model (like a Decision Tree or Random Forest) to determine which factors contribute most significantly to timely or delayed shipments, allowing the business to optimize its supply chain and improve customer satisfaction.
"""

print("Columns:", df1.columns.tolist())

print(df1.shape)

print(variabledescription)

"""**Distribution of major numerical columns**

The columns related to Financial Metrics—such as profit_per_order, sales_per_customer, order_item_discount, and order_profit_per_order—are the primary indicators of a shipment's economic outcome. Understanding the distribution of these values, particularly the profit and sales figures, is essential as they serve as vital inputs for predictive modeling (e.g., predicting profit) or as the basis for calculating key performance indicators (KPIs). Complementing the financial data is the Quantity/Volume metric, specifically order_item_quantity, which dictates the physical limitations and resource allocation for the shipment, influencing decisions related to packaging, transportation capacity, and ultimate logistics costs.
"""

numerical_columns=df1.select_dtypes(exclude='object').columns

major_numerical_col=['profit_per_order', 'sales_per_customer',
       'order_item_discount', 'order_item_discount_rate',
       'order_item_product_price', 'order_item_profit_ratio',
       'order_item_quantity', 'sales', 'order_item_total_amount',
       'order_profit_per_order',
       'product_price']

for col in major_numerical_col:
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # Histogram + KDE
    sns.histplot(df1[col], kde=True, ax=axes[0])
    axes[0].set_title(f'Histogram & KDE of {col}')

    # Boxplot
    sns.boxplot(x=df1[col], ax=axes[1])
    axes[1].set_title(f'Boxplot of {col}')

    plt.tight_layout()
    plt.savefig("static/plots/1_fig.png")
    plt.close()


# For object (categorical) columns, show top unique values and counts for a quick glance
print("\n counts of the categorical column and their unique value (top 10 shown):")
cat_cols = df1.select_dtypes(include="object").columns.tolist()
for col in cat_cols:
    print(f"\n--- {col} ---")
    display(df1[col].value_counts().head(10))

"""**Visuals for some major categorical columns**"""

cols = ['payment_type','customer_segment','market','order_region','order_status','shipping_mode']
for col in cols:
    sns.countplot(y=col,data=df1)
    plt.savefig("static/plots/2_fig.png")
    plt.close()

plt.figure(figsize=(8,15))
sns.countplot(y='category_name',data=df1)
plt.savefig("static/plots/3_fig.png")
plt.close()
"""# Understand the target (Label) variable

Markdown (explain):)
We saw a label column with values -1, 0, 1. We will create a binary delay_flag: 1 indicates delayed (label == -1), and 0 indicates not delayed (label == 0 or 1). Check class balance.
"""

# Check original label distribution
print("Original label value counts:")
display(df1['label'].value_counts())

# Create binary target
# 1 = Delayed (label == 1)
# 0 = On time or early (label == 0 or -1)
df1['delay_flag'] = df1['label'].apply(lambda x: 1 if x == 1 else 0)

# Check new target distribution
print("\nBinary delay_flag distribution:")
display(df1['delay_flag'].value_counts())
display(df1['delay_flag'].value_counts(normalize=True))

plt.figure(figsize=(4,4))
sns.countplot(y='delay_flag',data=df1)
plt.savefig("static/plots/4_fig.png")
plt.close()
"""# Data Cleaning

Check for missing values, duplicates, invalid datetimes, and fix datatypes (especially date columns). Drop or impute where appropriate.
"""

missing = df1.isnull().sum().sort_values(ascending=False)
print(missing.head())

n_dup=df1.duplicated().sum() #it has no duplicate value

# If there are duplicates, drop them (optional as per future data)

if n_dup > 0:
    df1 = df1.drop_duplicates()
    print("Dropped duplicate rows. New shape:", df1.shape)

# Convert date columns to datetime objects
df1["order_date"] = pd.to_datetime(df1["order_date"], utc=True).dt.tz_localize(None)
df1["shipping_date"] = pd.to_datetime(df1["shipping_date"], utc=True).dt.tz_localize(None)

# Ensure dates are proper datetime types
# Calculate delay in days and store it inside the dataframe
df1["shipping_delay_days"] = (df1["shipping_date"] - df1["order_date"]).dt.days

df1['shipping_delay_days'] = df1['shipping_delay_days'].astype(int)

# Check the new type
print(type(df1['shipping_delay_days']))

# Check that the new column is added
df1[["order_date", "shipping_date", "shipping_delay_days"]].head()

"""# Exploratory Data Analysis

# Understand geographical and customer-based patterns.

**1 Which countries generate the most orders?**
This helps us identify the primary geographical markets contributing to the business.
"""

orders_by_country = df1["customer_country"].value_counts()

plt.figure(figsize=(8,5))
sns.barplot(x=orders_by_country.values, y=orders_by_country.index, palette="Blues")
plt.title("Orders by Country")
plt.xlabel("Number of Orders")
plt.ylabel("Country")
plt.savefig("static/plots/5_fig.png")
plt.close()
"""**2 Which customer segments dominate each country?**"""

country_segment = df1.groupby(["customer_country", "customer_segment"])["customer_id"].count().reset_index()

plt.figure(figsize=(12,6))
sns.barplot(
    data=country_segment,
    x="customer_country",
    y="customer_id",
    hue="customer_segment",
    palette="tab10"
)
plt.title("Customer Segments by Country")
plt.ylabel("Order Count")
plt.xlabel("Country")
plt.xticks(rotation=20)
plt.savefig("static/plots/6_fig.png")
plt.close()
"""**Interpretation**

we can clearly see "consumer" customer segment is strongest in each country — e.g., some countries may be dominated by Corporate customers, while others are mostly Consumer buyers.Understanding this distinction allows businesses to tailor marketing strategies, promotional activities, and inventory stocking based on the customer composition in each country. Countries with high Corporate demand may require dedicated B2B logistics and priority shipping, while Consumer-heavy nations may benefit from price discounts and more flexible delivery windows.

**3.Relationship between customer segment and order quantity**
"""

plt.figure(figsize=(10,8))
sns.boxplot(
    data=df1,
    x="order_region",
    y="sales_per_customer",
    palette="viridis"
)
plt.title("Order Value Distribution by Region")
plt.xticks(rotation=35)
plt.savefig("static/plots/7_fig.png")
plt.close()
"""**Interpretation**

This analysis highlights the purchasing patterns of different customer segments. Corporate customers often place larger orders due to business needs, whereas Consumer customers typically place smaller, need-based orders. This distinction has direct implications for logistics planning—Corporate shipments may require bulk handling, optimized packaging, and special delivery scheduling.

The insight also helps in inventory forecasting. Product demand from large-quantity buyers can significantly impact warehouse availability and replenishment cycles, making it crucial to anticipate their order patterns.

## 2.Product Analysis

**2.1 Which product categories are preferred by different customer segments?**
"""

cat_segment = df1.groupby(["customer_segment", "category_name"])["order_id"].count().reset_index()
cat_segment
plt.figure(figsize=(14,6))
sns.barplot(
    data=cat_segment.sort_values("order_id", ascending=False).head(30),
    x="order_id",
    y="category_name",
    hue="customer_segment",
    palette="Set2"
)
plt.title("Top Product Categories by Customer Segment")
plt.xlabel("Order Count")
plt.ylabel("Product Category")
plt.savefig("static/plots/8_fig.png.png")
plt.close()
"""**interpretation**

This analysis reveals which customer groups prefer specific categories, providing actionable information for targeted sales strategies. If Corporate customers tend to purchase items like office supplies in high volumes, whereas Consumers gravitate towards lifestyle or personal-use categories, the business can tailor advertising campaigns accordingly.

Additionally, identifying the overlap or gaps between segment behaviors allows the company to diversify product offerings, identify cross-selling opportunities, and optimize category-specific stocking strategies.

**2.2 Which products generate repeated large orders?**
"""

bulk_products = (
    df1[df1["order_item_quantity"] > 3].groupby("category_name")["order_item_quantity"].count().sort_values(ascending=False).head(10))

plt.figure(figsize=(10,5))
sns.barplot(y=bulk_products.index, x=bulk_products.values, palette="coolwarm")
plt.title("Products Frequently Bought in Bulk")
plt.xlabel("Number of Bulk Orders")
plt.ylabel("Product Category")
plt.savefig("static/plots/9_fig.png")
plt.close()
"""**Interpretation**

By isolating products frequently bought in bulk, this analysis identifies categories that drive large operational loads. These products may require special packaging, pallet-based handling, or dedicated warehouse space.

Bulk-order categories are critical for forecasting peak-season demands, as they may heavily influence storage costs, shipping capacities, and replenishment cycles. Ensuring consistent supply for these categories can prevent stockouts and sustain customer satisfaction for high-volume buyers such as wholesalers or corporate clients.

# 3 Shipping Analysis

**3.1 Association between shipping mode and customer segment**
"""

sgmt_spng = df1.groupby(["customer_segment", "shipping_mode"])["order_id"].count().reset_index()

plt.figure(figsize=(10,5))
sns.barplot(
    data=sgmt_spng,
    x="customer_segment",
    y="order_id",
    hue="shipping_mode",
    palette="tab10"
)
plt.title("Shipping Modes Preferred by Customer Segments")
plt.ylabel("Order Count")
plt.savefig("static/plots/10_fig.png")
plt.close()
"""Different customer segments may prefer different shipping speeds and price points. Corporate customers may favor expedited shipping due to time-sensitive business needs, while Consumers may prioritize affordability over speed.

Understanding these preferences helps logistics teams allocate capacity more efficiently. It also enables the company to position different shipping options strategically—for example, promoting express delivery to high-value Corporate customers or offering economy plans to budget-conscious Consumers.

**q3.2 What are the Average Order Value by Shipping Mode**
"""

# Calculate mean sales per shipping mode
avg_value_by_ship = df1.groupby("shipping_mode")["sales_per_customer"].mean().sort_values()

plt.figure(figsize=(10,5))
ax = sns.barplot(
    x=avg_value_by_ship.index,
    y=avg_value_by_ship.values,
    palette="PuRd"
)

plt.title("Average Order Value by Shipping Mode")
plt.xlabel("Shipping Mode")
plt.ylabel("Average Order Value ($)")
plt.xticks(rotation=20)

# Add values on top of bars
for i, v in enumerate(avg_value_by_ship.values):
    ax.text(i, v + 2, f"${v:.2f}", ha="center", fontsize=10, fontweight="bold")

plt.savefig("static/plots/11_fig.png")
plt.close()
"""**Interpretation**

Thus, this insight supports strategic investments in the shipping modes that matter most to the business.

This comparison chart is crucial for aligning logistics resources with customer value, as it directly compares the average monetary value of orders against their corresponding shipping modes. High average order values in premium categories like "First Class" or "Same Day" indicate that customers placing large or urgent orders prioritize speed and reliability, confirming the need for priority processing and strict Service Level Agreement (SLA) adherence for these high-revenue shipments. Conversely, a lower average order value for "Standard Class" suggests this channel primarily serves price-sensitive consumers and cost-effective orders, making it the ideal target for cost-optimization efforts within the shipping strategy. This distinction allows the logistics team to efficiently allocate resources, ensuring that any potential delays in high-value shipping modes, which carry the highest financial risk, are rigorously managed.

# Order Volume Analysis

**Number of Orders by Customer Segment**
"""

# Number of orders by customer segment
temp = df1['customer_segment'].value_counts()

plt.figure(figsize=(8, 6))
colors = sns.color_palette("Set2", len(temp))  # different color for each bar

ax = sns.barplot(
    x=temp.index,
    y=temp.values,
    palette=colors
)

# Annotate bars with counts
for p in ax.patches:
    height = p.get_height()
    ax.annotate(
        f'{int(height)}',
        (p.get_x() + p.get_width() / 2., height),
        ha='center', va='bottom',
        fontsize=11, fontweight='bold',
        xytext=(0, 5),
        textcoords='offset points'
    )

plt.xlabel("Customer Segment", fontsize=12)
plt.ylabel("Number of Orders", fontsize=12)
plt.title("Orders Distribution by Customer Segment", fontsize=14, fontweight='bold')

plt.xticks(rotation=30)
plt.tight_layout()
plt.savefig("static/plots/12_fig.png")
plt.close()
"""**What are the moode of payment for each order**"""

# Count payment types
temp = df1['payment_type'].value_counts().reset_index()
temp.columns = ['payment_type', 'count']  # rename columns

# Function for custom labels
def func(pct, allvalues):
    absolute = int(pct/100.*sum(allvalues))
    return f'{absolute:,}\n({pct:.1f}%)'

# Enhanced Pie Chart
plt.figure(figsize=(8, 8))
plt.pie(
    x=temp['count'],
    labels=temp['payment_type'],
    autopct=lambda pct: func(pct, temp['count']),
    colors=sns.color_palette('Set2', len(temp)),  # beautiful multi-color palette
    startangle=140,
    wedgeprops={'edgecolor': 'white', 'linewidth': 1}
)

plt.title("Distribution of Orders by Payment Type", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig("static/plots/13_fig.png")
plt.close()
"""**Interpretation**

The pie chart reveals the distribution of customer payment preferences across all orders.

The largest slice corresponds to the most frequently used payment method, showing that customers predominantly rely on this mode of payment.

Payment types with smaller slices indicate lower adoption and may represent:

Additional friction for customers (e.g., bank transfer, COD)

Longer processing times

Higher abandonment or failure rates

This insight is important because payment method choice influences order processing speed, which can ultimately affect delivery outcomes.

# 4.Delay Analysis

**4.1 What percentage of orders are delayed?**
"""

delay_counts = df1["delay_flag"].value_counts(normalize=True) * 100

plt.figure(figsize=(5,5))
plt.pie(delay_counts, labels=["On Time","Delayed"], autopct="%1.1f%%", colors=["#6ECF68","#FF6B6B"])
plt.title("Overall Delay Percentage")
plt.savefig("static/plots/14_fig.png")
plt.close()
"""**4.2 Delay rate by region + shipping mode**"""

region_mode_delay = df1.groupby(["order_region", "shipping_mode"])["delay_flag"].mean().reset_index()

plt.figure(figsize=(15,6))
sns.barplot(
    data=region_mode_delay,
    x="order_region",
    y="delay_flag",
    hue="shipping_mode",
    palette="tab20"
)
plt.title("Delay Rate by Region & Shipping Mode")
plt.ylabel("Delay Rate")
plt.xlabel("Region")
plt.xticks(rotation=30)
plt.savefig("static/plots/15_fig.png")
plt.close()
"""**Impact Business Conclusion**

The analysis clearly shows that delivery performance is not only dependent on the shipping mode but also heavily influenced by region.
Premium services (First Class, Same Day) are consistently failing expectations, while Standard and Second-Class are more reliable.

This means:

The company should re-evaluate First-Class and Same-Day delivery promises, especially in developing regions.

Logistics partners in high-delay regions should be reviewed or replaced.

Pricing and service-level agreements (SLAs) for fast shipping need adjustment.

Future predictive modeling should include both region and shipping_mode as critical features because of their strong influence on delays.

**4.3 What proportion of delays that are within operational control (treatable) versus those caused by external or customer-side issues (non-treatable).?**
"""

non_treatable_statuses = ["PENDING_PAYMENT", "On_Hold"]

treatable_delay = df1["order_status"].apply(
    lambda x: "Non-Treatable Delay" if x in non_treatable_statuses else "Treatable Delay"
)

treatable_counts = treatable_delay.value_counts(normalize=True) * 100

plt.figure(figsize=(6,6))
plt.pie(
    treatable_counts,
    labels=treatable_counts.index,
    autopct="%1.1f%%",
    colors=["#FF6B6B", "#6ECF6A"],
    startangle=90
)
plt.title("Proportion of Treatable vs Non-Treatable Delays")
plt.savefig("static/plots/16_fig.png")
plt.close()
"""**Interpretation**
The chart shows the proportion of delays that are within operational control (75.4%) versus those caused by external or customer-side issues (24.6%).

Treatable delays represent shipment delays due to logistics inefficiencies such as slow processing, routing issues, backlog, transportation delays, or failed SLA compliance. These are delays the company can reduce through operational improvements.

Non-treatable delays occur due to customer actions (e.g., late payment) or business conditions outside logistics control (e.g., orders on hold). These cannot be solved through shipping optimizations.

This breakdown is crucial because it helps the company understand how much of the delay problem can realistically be fixed using machine learning or process improvement, and how much is outside operational control.

# Model building

# Data preprocessing

**Reduce Cardinality of High-Cardinality Categorical Columns**

Some categorical columns have hundreds or thousands of unique values (cities, states, zip codes).
If we one-hot encode them, we will create too many columns, which:

Makes the model slow

Causes overfitting

Adds noise instead of learning

So we group rare categories into “Others”.
Categories that appear very few times do not contribute meaningful patterns.
They are grouped into a general category “Others” to reduce overfitting and improve model stability.
"""

model_data = df1.copy()

# REDUCING CARDINALITY FOR CATEGORICAL VARIABLES
# (same logic as the original code)

# 1. customer_city
customer_city_count = model_data['customer_city'].value_counts()
city_less_50 = customer_city_count[customer_city_count < 50].index.tolist()
model_data['customer_city'] = model_data['customer_city'].apply(
    lambda x: 'Others' if x in city_less_50 else x
)

# 2. customer_state
customer_state_count = model_data['customer_state'].value_counts()
state_less_50 = customer_state_count[customer_state_count < 50].index.tolist()
model_data['customer_state'] = model_data['customer_state'].apply(
    lambda x: 'Others' if x in state_less_50 else x
)

# 3. order_city
order_city_count = model_data['order_city'].value_counts()
order_city_less_50 = order_city_count[order_city_count < 50].index.tolist()
model_data['order_city'] = model_data['order_city'].apply(
    lambda x: 'Others' if x in order_city_less_50 else x
)

# 4. order_country
order_country_count = model_data['order_country'].value_counts()
country_less_50 = order_country_count[order_country_count < 50].index.tolist()
model_data['order_country'] = model_data['order_country'].apply(
    lambda x: 'Others' if x in country_less_50 else x
)

# 5. order_region
order_region_count = model_data['order_region'].value_counts()
region_less_100 = order_region_count[order_region_count < 100].index.tolist()
model_data['order_region'] = model_data['order_region'].apply(
    lambda x: 'Others' if x in region_less_100 else x
)

# 6. order_state
order_state_count = model_data['order_state'].value_counts()
state_less_50 = order_state_count[order_state_count < 50].index.tolist()
model_data['order_state'] = model_data['order_state'].apply(
    lambda x: 'Others' if x in state_less_50 else x
)

#to see the chabnges
categorical_cols=model_data.select_dtypes(include=['object'])
for cols in categorical_cols:
    print(model_data[cols].value_counts())
    print()

"""**Chi-Square Test (Remove Categorical Variables Not Related to Target)**
The Chi-square test of independence was used to evaluate whether each categorical feature had a statistically significant association with the target variable (label).
Following the methodology of the benchmark study, only key categorical variables relevant to customer, order, and shipping information were included.
Variables with p-value ≥ 0.1 were considered independent of the target and therefore dropped.
This step removes noise, reduces dimensionality, and ensures only meaningful categorical predictors are used for model building.
"""

# Select categorical columns except target
cat_df = model_data.select_dtypes(include=['object']).copy()
cat_df['delay_flag'] = model_data['delay_flag']

from scipy.stats import chi2_contingency

independent_cols = []

for col in cat_df.columns:
    if col == "delay_flag":  # skip target column
        continue

    observed_freq = pd.crosstab(cat_df[col], cat_df['delay_flag'])
    chi = chi2_contingency(observed_freq)

    if chi.pvalue < 0.1:
        print(f"{col} is DEPENDENT on delay_flag (p-value = {chi.pvalue:.4f})")
    else:
        print(f"{col} is INDEPENDENT — will be DROPPED (p-value = {chi.pvalue:.4f})")
        independent_cols.append(col)

    print()

print("Columns to drop:", independent_cols)

indepet_cols = ['category_name', 'customer_city', 'customer_country', 'customer_segment',
                'customer_state', 'department_name', 'market', 'order_city', 'order_country', 'order_region',
                'order_state', 'order_status', 'product_name']

'''Drop independent categorical columns. we are keeping "payment_type" column becaause it dependent when we keep the target as it is "0,1,-1"
but for the model building we are keeping it [-1,0] as 0 i.e on time and 1 as late.'''

model_data.drop(columns=indepet_cols, inplace=True,errors='ignore')
print(model_data.select_dtypes(include=['object'])) # this is the only categorical column left for after removing the independent variable
print(model_data.shape)
print(df1.shape)

"""**Droping unwanted columns**

IDs, card/product identifiers, and raw zipcode typically carry no predictive signal and often leak information or explode dimensionality. We also drop the raw label (keep delay_flag as the target).
"""

cols_to_drop = [
    "category_id", "customer_id", "customer_zipcode", "department_id",
    "order_customer_id", "order_id", "order_item_cardprod_id", "order_item_id",
    "product_card_id", "product_category_id", "label"]

# Drop them if present
model_data.drop(columns=cols_to_drop, inplace=True, errors="ignore")


print("Remaining columns:", model_data.columns.tolist())
model_data.shape

# Identify numeric columns in the current dataframe
num_cols = model_data.select_dtypes(include=["int64","float64"]).columns.tolist()
# exclude the target if it is numeric
if "delay_flag" in num_cols:
    num_cols.remove("delay_flag")

print("Numeric columns to check:", num_cols)

"""# Correlation Analysis and Variance Inflation Factor (VIF)

We examine numeric features for high correlation and multicollinearity because variables that convey the same information harm model stability.

The heatmap helps visually spot pairs or groups of highly correlated variables (|r| ≈ 0.8+), which we will then quantify with VIF.
"""

#Correlation heatmap
plt.figure(figsize=(12,10))
sns.heatmap(model_data[num_cols].corr(), annot=True, fmt=".2f", cmap="coolwarm", center=0)
plt.title("Correlation matrix — numeric features")
plt.savefig("static/plots/20_fig.png")
plt.close()
"""**Iterative VIF removal**

VIF > 5 (common threshold) indicates problematic multicollinearity. We iteratively drop the highest-VIF feature until all VIFs ≤ 5.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd
import numpy as np

# Prepare DataFrame for VIF (drop rows with NaN in numeric cols to compute VIF reliably)
X_vif = model_data[num_cols].copy().dropna().reset_index(drop=True)

# Add constant for statsmodels VIF calculation
X_vif_for_vif = X_vif.copy()
X_vif_for_vif["const"] = 1

vif_threshold = 5.0
removed_cols_vif = []

while True:
    vif_series = pd.Series(
        [variance_inflation_factor(X_vif_for_vif.values, i)
         for i in range(X_vif_for_vif.shape[1])],
        index=X_vif_for_vif.columns
    ).drop("const", errors="ignore")

    max_vif = vif_series.max()
    if max_vif > vif_threshold:
        col_to_drop = vif_series.idxmax()
        removed_cols_vif.append((col_to_drop, float(max_vif)))
        # Drop from both the VIF DF and the working model_data if present
        X_vif_for_vif.drop(columns=[col_to_drop], inplace=True)
        X_vif.drop(columns=[col_to_drop], inplace=True)
    else:
        break

print("VIF-removed columns:", removed_cols_vif)
print("col after Vif ", X_vif.columns.tolist())

# Updating model_data by dropping the removed VIF columns (if they exist)
cols_removed = [c for c, _ in removed_cols_vif]
model_data.drop(columns=cols_removed, inplace=True, errors="ignore")

final_numeric = model_data.select_dtypes(include=["int64","float64"]).columns.tolist()
# remove target if present
if "delay_flag" in final_numeric:
    final_numeric.remove("delay_flag")

final_categorical = model_data.select_dtypes(include=["object","category"]).columns.tolist()

print("Final numeric features to use:", final_numeric)
print("Final categorical features to use:", final_categorical)
print("Target column:", "delay_flag" if "delay_flag" in model_data.columns else "MISSING")

from sklearn.model_selection import train_test_split

# Define X and y
X = model_data.drop("delay_flag", axis=1)
y = model_data["delay_flag"]

# Train-test split (stratified to maintain 0/1 ratio)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train size:", X_train.shape)
print("Test size:", X_test.shape)

numeric_features = [
    'sales_per_customer','latitude','longitude','order_item_discount',
    'order_item_discount_rate','order_item_profit_ratio',
    'order_item_quantity','order_profit_per_order','shipping_delay_days'
]

categorical_features = ['payment_type','shipping_mode']



from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ]
)

"""# Models

**Model 1 — Gradient Boosting**
"""

from sklearn.ensemble import GradientBoostingClassifier

gb_model = Pipeline(steps=[
    ('preprocessing', preprocessor),
    ('classifier', GradientBoostingClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=3,
        random_state=42
    ))
])

gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)
y_proba_gb = gb_model.predict_proba(X_test)[:,1]

"""**Model 2 — XGBoost**"""

from xgboost import XGBClassifier

xgb_model = Pipeline(steps=[
    ('preprocessing', preprocessor),
    ('classifier', XGBClassifier(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric='logloss',
        random_state=42
    ))
])

xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
y_proba_xgb = xgb_model.predict_proba(X_test)[:,1]

"""**Evaluation Function (Confusion Matrix , ROC Curve and AUC)**"""

from sklearn.metrics import (
    confusion_matrix, classification_report, roc_auc_score,
    roc_curve, ConfusionMatrixDisplay
)
import matplotlib.pyplot as plt

def evaluate_model(name, y_test, y_pred, y_proba):

    print(f"Model: {name}")
    print("================================================")

    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(cm)
    disp.plot(cmap="Blues")
    plt.title(f"Confusion Matrix: {name}")
    plt.savefig("static/plots/17_fig.png")
    plt.close()
    # ROC Curve
    auc = roc_auc_score(y_test, y_proba)
    fpr, tpr, _ = roc_curve(y_test, y_proba)

    plt.figure(figsize=(6,4))
    plt.plot(fpr, tpr, label=f"AUC = {auc:.3f}")
    plt.plot([0,1],[0,1],'--',color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC Curve: {name}")
    plt.legend()
    plt.savefig("static/plots/18_fig.png")
    plt.close()
    print(f"AUC Score: {auc:.3f}")

evaluate_model("Gradient Boosting", y_test, y_pred_gb, y_proba_gb)
evaluate_model("XGBoost", y_test, y_pred_xgb, y_proba_xgb)

"""**Selecting the Best Model**
we find that both Gradirnt Boosting and XGBoost are giving the simillar accuracy.
"""

auc_gb = roc_auc_score(y_test, y_proba_gb)
auc_xgb = roc_auc_score(y_test, y_proba_xgb)

print("Gradient Boosting AUC:", auc_gb)
print("XGBoost AUC:", auc_xgb)

best_model = "XGBoost" if auc_xgb > auc_gb else "Gradient Boosting"
print("BEST MODEL:", best_model)

#save the pickle file
best_pipeline = xgb_model if auc_xgb > auc_gb else gb_model
joblib.dump(best_pipeline, "ml_model.pkl")
print("Best model saved as ml_model.pkl")

